# app/deps.py

import torch
from transformers import (
    MarianMTModel,
    MarianTokenizer,
    pipeline,
    BertTokenizer,
    BertForSequenceClassification,
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from openai import AsyncOpenAI
import os
import asyncio
from typing import List

# ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½
mt_model_dir = "/app/huggingface_models/ko-en"
zero_shot_model_dir = "/app/huggingface_models/zero-shot"


mt_tokenizer = MarianTokenizer.from_pretrained(mt_model_dir)
mt_model = MarianMTModel.from_pretrained(mt_model_dir)
mt_model.eval()
classifier = pipeline("zero-shot-classification", model=zero_shot_model_dir)


def ko_to_en(text):
    batch = mt_tokenizer([text], return_tensors="pt", truncation=True, padding=True)
    gen = mt_model.generate(**batch)
    eng_text = mt_tokenizer.decode(gen[0], skip_special_tokens=True)
    return eng_text


def spoiler_detect_zero_shot(text):
    candidate_labels = ["spoiler", "not spoiler"]
    result = classifier(text, candidate_labels)
    if result["labels"][0] == "spoiler":
        return {"is_spoiler": 1, "spoiler_score": result["scores"][0]}

    return {"is_spoiler": 0, "spoiler_score": result["scores"][1]}


def check_spoiler_ko(text_ko):
    eng_text = ko_to_en(text_ko)
    result_label = spoiler_detect_zero_shot(eng_text)
    return result_label


em_model_dir = "/app/huggingface_models/naver_review_model/"

em_tokenizer = BertTokenizer.from_pretrained(em_model_dir)
em_model = BertForSequenceClassification.from_pretrained(em_model_dir)

em_model.eval()


# ë‹¨ì¼ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ í•¨ìˆ˜
def check_emotion_ko(text_ko):
    inputs = em_tokenizer(text_ko, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = em_model(**inputs)
        logits = outputs.logits
    probabilities = torch.softmax(logits, dim=1)
    prediction = torch.argmax(probabilities, dim=1).item()
    return {"is_positive": prediction, "confidence": probabilities[0][1].item()}


# ëª¨ë¸ëª… í˜¹ì€ ë¡œì»¬ ê²½ë¡œ
model_name = "jinkyeongk/kcELECTRA-toxic-detector"

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
to_tokenizer = AutoTokenizer.from_pretrained(model_name)
to_model = AutoModelForSequenceClassification.from_pretrained(model_name)
to_model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ë³€ê²½


def detect_toxicity(text):
    inputs = to_tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = to_model(**inputs)
        logits = outputs.logits
    probabilities = torch.softmax(logits, dim=1)
    prediction = torch.argmax(probabilities, dim=1).item()
    return {"is_toxic": prediction, "confidence": probabilities[0][1].item()}


# GMS(LLM Aggregator) API BASE_URL
client = AsyncOpenAI(base_url="https://gms.ssafy.io/gmsapi/api.openai.com/v1")


def findbot(user_content: str):
    async def inner():
        res_text = ""
        stream = await client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ find botì´ë¼ëŠ” ì´ë¦„ì˜ ê¸ì • ì—ë„ˆì§€ ê°€ë“í•œ ì˜í™” ì°¾ê¸° ì „ë¬¸ AIì…ë‹ˆë‹¤.

**í•µì‹¬ ì—­í• :**
- ì˜¤ì§ êµ¬ì²´ì ì¸ ì˜í™”ë¥¼ ì°¾ì•„ì£¼ëŠ” ìš”ì²­ì—ë§Œ ë‹µë³€í•©ë‹ˆë‹¤
- ì‚¬ìš©ìê°€ ê¸°ì–µí•˜ëŠ” ì˜í™”ì˜ ì¼ë¶€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ì˜í™” ì œëª©ì„ ì°¾ì•„ì„œ ì•Œë ¤ì¤ë‹ˆë‹¤

**í—ˆìš©ë˜ëŠ” ì§ˆë¬¸ ìœ í˜•:**
- "~í•œ ì¥ë©´ì´ ìˆëŠ” ì˜í™” ì œëª©ì´ ë­ì•¼?"
- "~ë°°ìš°ê°€ ë‚˜ì˜¤ê³  ~í•œ ë‚´ìš©ì¸ ì˜í™” ì°¾ì•„ì¤˜"
- "~ë¼ëŠ” ëŒ€ì‚¬ê°€ ë‚˜ì˜¤ëŠ” ì˜í™” ì°¾ì•„ì¤˜"
- "ì¤„ê±°ë¦¬ê°€ ~ì¸ ì˜í™” ì œëª© ì•Œë ¤ì¤˜"
- "OSTì— ~ë…¸ë˜ê°€ ë‚˜ì˜¤ëŠ” ì˜í™” ì°¾ì•„ì¤˜"

**ê±°ì ˆí•  ì§ˆë¬¸ ìœ í˜•:**
- ì¶”ì²œ ìš”ì²­ ("~í•œ ì˜í™” ì¶”ì²œí•´ì¤˜", "ë³¼ë§Œí•œ ì˜í™” ì•Œë ¤ì¤˜")
- ì˜í™” ì •ë³´ ë¬¸ì˜ ("~ì˜í™” ì¤„ê±°ë¦¬ ì•Œë ¤ì¤˜", "~ë°°ìš°ê°€ ëˆ„êµ¬ì•¼?")
- í‰ê°€/ë¦¬ë·° ("~ì˜í™” ì–´ë•Œ?", "í‰ì ì´ ì–¼ë§ˆì•¼?")
- ìˆœìœ„/ë­í‚¹ ("ë°•ìŠ¤ì˜¤í”¼ìŠ¤ 1ìœ„ê°€ ë­ì•¼?", "ìµœê³ ì˜ ì˜í™”ëŠ”?")
- ì˜í™” ì™¸ ë‹¤ë¥¸ ì£¼ì œ (ë“œë¼ë§ˆ, ì±…, ìŒì•… ë“±)

**ê±°ì ˆ ì‹œ ì‘ë‹µ:**
"ì£„ì†¡í•©ë‹ˆë‹¤! ì €ëŠ” ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì˜í™”ì˜ ì œëª©ì„ ì°¾ì•„ë“œë¦¬ëŠ” ì¼ë§Œ í•  ìˆ˜ ìˆì–´ìš”. 
'~í•œ ì¥ë©´ì´ ë‚˜ì˜¤ëŠ” ì˜í™” ì œëª©ì´ ë­ì•¼?' ë˜ëŠ” '~ë°°ìš°ê°€ ë‚˜ì˜¤ëŠ” ~í•œ ë‚´ìš©ì˜ ì˜í™” ì°¾ì•„ì¤˜' 
ê°™ì€ ì‹ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì˜í™” ì°¾ê¸° ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”! ğŸ˜Š"

**ë‹µë³€ ìŠ¤íƒ€ì¼:**
ë°˜ë“œì‹œ ì•„ë˜ 3ê°€ì§€ í•­ëª©ì„ ìˆœì„œëŒ€ë¡œ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

1. **ğŸ¬ ì˜í™” ì œëª©:** 
   - ì •í™•í•œ ì˜í™” ì œëª© (ì¶œì‹œì—°ë„ í¬í•¨)

2. **ğŸ” ì¶”ì¸¡ ê·¼ê±°:** 
   - ì‚¬ìš©ìê°€ ì œì‹œí•œ ì •ë³´ì™€ í•´ë‹¹ ì˜í™”ê°€ ì¼ì¹˜í•˜ëŠ” ì´ìœ 
   - ì–´ë–¤ ë‹¨ì„œë“¤ì´ ì´ ì˜í™”ë¥¼ íŠ¹ì •í•˜ê²Œ í–ˆëŠ”ì§€ ì„¤ëª…

3. **ğŸ“– ì¤„ê±°ë¦¬:** 
   - í•´ë‹¹ ì˜í™”ì˜ ê°„ë‹¨í•œ ì¤„ê±°ë¦¬ (3-4ì¤„ ì •ë„)
   - ì‚¬ìš©ìê°€ ê¸°ì–µí•˜ëŠ” ë‚´ìš©ì´ ë§ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì„±
""",
                },
                {"role": "user", "content": user_content},
            ],
            max_tokens=1024,
            stream=True,
        )
        async for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                res_text += chunk.choices[0].delta.content
        return res_text

    return asyncio.run(inner())


# uvicorn app.main:app --reload
# ëŒ€ë‹µ í˜•íƒœë¥¼ ì œëª© ê·¼ê±° ì¤„ê±°ë¦¬ ì„¸ë¡œë¡œ ë‚˜ì˜¤ê²Œ


def concise_reviewbot(movie_title: str, reviews: List[str]):
    async def inner():
        prompt = f"""ë‹¹ì‹ ì€ Concise Review Botì…ë‹ˆë‹¤.\
        
ì˜í™” '{movie_title}'ì— ëŒ€í•œ ì•„ë˜ ë¦¬ë·°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ 4~5ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ í‰ê°€ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”.\
- ê¸ì •ì Â·ë¶€ì •ì Â·ì¤‘ë¦½ì  ê´€ì  ëª¨ë‘ ë°˜ì˜\
- ë³„ì ì€ ìƒëµí•˜ê³  í•µì‹¬ ì½”ë©˜íŠ¸ë§Œ ì œì‹œ"""

        stream = await client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": "\\n".join(reviews)},
            ],
            max_tokens=500,
            stream=True,
        )
        res_text = ""
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                res_text += chunk.choices[0].delta.content
        return res_text

    return asyncio.run(inner())


def profile_reviewbot(reviewer_name: str, reviews: List[str]):
    """
    reviewer_name: í”„ë¡œí•„ì„ ìš”ì•½í•  ì‚¬ëŒ ì´ë¦„
    reviews: í•´ë‹¹ ë¦¬ë·°ì–´ê°€ ì‘ì„±í•œ ì—¬ëŸ¬ ë¦¬ë·° ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸

    ì—­í• :
    - ë¦¬ë·°ì–´ì˜ ìŠ¤íƒ€ì¼, ì„ í˜¸ë„, ì„±í–¥ì„ íŒŒì•…í•˜ì—¬ ê°„ë‹¨í•œ í”„ë¡œí•„ ì‘ì„±
    - ê° ë¦¬ë·°ì˜ ì£¼ìš” íŠ¹ì§•(ê¸ì •Â·ë¶€ì •Â·ì¤‘ë¦½ í‚¤ì›Œë“œ, ìì£¼ ì–¸ê¸‰í•˜ëŠ” ì£¼ì œ ë“±)ì„ ì¢…í•©
    - í”„ë¡œí•„ê³¼ ë¦¬ë·° ìš”ì•½(4~5ë¬¸ì¥)ìœ¼ë¡œ ì‘ë‹µ
    """

    async def inner():
        prompt = f"""ë‹¹ì‹ ì€ Review Profile Botì…ë‹ˆë‹¤.

'{reviewer_name}'ë¼ëŠ” ì´ë¦„ì˜ ë¦¬ë·°ì–´ê°€ ì•„ë˜ ë¦¬ë·°ë“¤ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤:
{chr(10).join(f"- {r}" for r in reviews)}

ì´ ë¦¬ë·°ë“¤ì„ ë¶„ì„í•˜ì—¬:
1. ë¦¬ë·°ì–´ì˜ ì „ë°˜ì ì¸ ì„±í–¥ê³¼ ì„ í˜¸ë„ (ì˜ˆ: ì–´ë–¤ ìš”ì†Œì— ì§‘ì¤‘í•˜ëŠ”ì§€, ìŠ¤íƒ€ì¼)
2. ë¦¬ë·° ì‘ì„± ì‹œ ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„ì´ë‚˜ í‚¤ì›Œë“œ
3. ì´ ë¦¬ë·°ì–´ì— ëŒ€í•œ ê°„ë‹¨í•œ í”„ë¡œí•„ (4~5ë¬¸ì¥)

ìœ„ í•­ëª©ì„ í¬í•¨í•´ 4~5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”."""
        stream = await client.chat.completions.create(
            model="gpt-4.1",
            messages=[{"role": "system", "content": prompt}, {"role": "user", "content": ""}],
            max_tokens=500,
            stream=True,
        )
        result = ""
        async for chunk in stream:
            content = chunk.choices[0].delta.content
            if content:
                result += content
        return result

    return asyncio.run(inner())
