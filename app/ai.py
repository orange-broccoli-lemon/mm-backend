# app/ai.py

import torch
from transformers import (
    MarianMTModel,
    MarianTokenizer,
    pipeline,
    BertTokenizer,
    BertForSequenceClassification,
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from openai import AsyncOpenAI
import os
import asyncio
from typing import List

# ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½
mt_model_dir = "/app/huggingface_models/ko-en"
zero_shot_model_dir = "/app/huggingface_models/zero-shot"


mt_tokenizer = MarianTokenizer.from_pretrained(mt_model_dir)
mt_model = MarianMTModel.from_pretrained(mt_model_dir)
mt_model.eval()
classifier = pipeline("zero-shot-classification", model=zero_shot_model_dir)


def ko_to_en(text):
    batch = mt_tokenizer([text], return_tensors="pt", truncation=True, padding=True)
    gen = mt_model.generate(**batch)
    eng_text = mt_tokenizer.decode(gen[0], skip_special_tokens=True)
    return eng_text


def spoiler_detect_zero_shot(text):
    candidate_labels = ["spoiler", "not spoiler"]
    result = classifier(text, candidate_labels)
    if result["labels"][0] == "spoiler":
        return {"is_spoiler": 1, "spoiler_score": result["scores"][0]}

    return {"is_spoiler": 0, "spoiler_score": result["scores"][1]}


def check_spoiler_ko(text_ko):
    eng_text = ko_to_en(text_ko)
    result_label = spoiler_detect_zero_shot(eng_text)
    return result_label


em_model_dir = "/app/huggingface_models/naver_review_model/"

em_tokenizer = BertTokenizer.from_pretrained(em_model_dir)
em_model = BertForSequenceClassification.from_pretrained(em_model_dir)

em_model.eval()


# ë‹¨ì¼ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ í•¨ìˆ˜
def check_emotion_ko(text_ko):
    inputs = em_tokenizer(text_ko, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = em_model(**inputs)
        logits = outputs.logits
    probabilities = torch.softmax(logits, dim=1)
    prediction = torch.argmax(probabilities, dim=1).item()
    return {"is_positive": prediction, "confidence": probabilities[0][1].item()}


# ëª¨ë¸ëª… í˜¹ì€ ë¡œì»¬ ê²½ë¡œ
model_name = "jinkyeongk/kcELECTRA-toxic-detector"

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
to_tokenizer = AutoTokenizer.from_pretrained(model_name)
to_model = AutoModelForSequenceClassification.from_pretrained(model_name)
to_model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ë³€ê²½


def detect_toxicity(text):
    inputs = to_tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = to_model(**inputs)
        logits = outputs.logits
    probabilities = torch.softmax(logits, dim=1)
    prediction = torch.argmax(probabilities, dim=1).item()
    return {"is_toxic": prediction, "confidence": probabilities[0][1].item()}


# GMS(LLM Aggregator) API BASE_URL
client = AsyncOpenAI(base_url="https://gms.ssafy.io/gmsapi/api.openai.com/v1")


async def findbot(user_content: str):
    system_prompt = """ë‹¹ì‹ ì€ find botì´ë¼ëŠ” ì´ë¦„ì˜ ê¸ì • ì—ë„ˆì§€ ê°€ë“í•œ ì˜í™” ì°¾ê¸° ì „ë¬¸ AIì…ë‹ˆë‹¤.

í•µì‹¬ ì—­í• :
- ì˜¤ì§ êµ¬ì²´ì ì¸ ì˜í™”ë¥¼ ì°¾ì•„ì£¼ëŠ” ìš”ì²­ì—ë§Œ ë‹µë³€í•©ë‹ˆë‹¤
- ì‚¬ìš©ìê°€ ê¸°ì–µí•˜ëŠ” ì˜í™”ì˜ ì¼ë¶€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ì˜í™” ì œëª©ì„ ì°¾ì•„ì„œ ì•Œë ¤ì¤ë‹ˆë‹¤

í—ˆìš©ë˜ëŠ” ì§ˆë¬¸ ìœ í˜•:
- "~í•œ ì¥ë©´ì´ ìˆëŠ” ì˜í™” ì œëª©ì´ ë­ì•¼?"
- "~ë°°ìš°ê°€ ë‚˜ì˜¤ê³  ~í•œ ë‚´ìš©ì¸ ì˜í™” ì°¾ì•„ì¤˜"
- "~ë¼ëŠ” ëŒ€ì‚¬ê°€ ë‚˜ì˜¤ëŠ” ì˜í™” ì°¾ì•„ì¤˜"
- "ì¤„ê±°ë¦¬ê°€ ~ì¸ ì˜í™” ì œëª© ì•Œë ¤ì¤˜"
- "OSTì— ~ë…¸ë˜ê°€ ë‚˜ì˜¤ëŠ” ì˜í™” ì°¾ì•„ì¤˜"

ê±°ì ˆí•  ì§ˆë¬¸ ìœ í˜•:
- ì¶”ì²œ ìš”ì²­ ("~í•œ ì˜í™” ì¶”ì²œí•´ì¤˜", "ë³¼ë§Œí•œ ì˜í™” ì•Œë ¤ì¤˜")
- ì˜í™” ì •ë³´ ë¬¸ì˜ ("~ì˜í™” ì¤„ê±°ë¦¬ ì•Œë ¤ì¤˜", "~ë°°ìš°ê°€ ëˆ„êµ¬ì•¼?")
- í‰ê°€/ë¦¬ë·° ("~ì˜í™” ì–´ë•Œ?", "í‰ì ì´ ì–¼ë§ˆì•¼?")
- ìˆœìœ„/ë­í‚¹ ("ë°•ìŠ¤ì˜¤í”¼ìŠ¤ 1ìœ„ê°€ ë­ì•¼?", "ìµœê³ ì˜ ì˜í™”ëŠ”?")
- ì˜í™” ì™¸ ë‹¤ë¥¸ ì£¼ì œ (ë“œë¼ë§ˆ, ì±…, ìŒì•… ë“±)

ì‘ë‹µ í˜•ì‹:
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

ì„±ê³µ ì‹œ:
{
  "success": true,
  "title": "ì˜í™” ì œëª© (ì¶œì‹œì—°ë„)",
  "movie_id": TMDB_ID_ìˆ«ì,
  "reason": "ì¶”ì¸¡ ê·¼ê±° - ì‚¬ìš©ìê°€ ì œì‹œí•œ ì •ë³´ì™€ í•´ë‹¹ ì˜í™”ê°€ ì¼ì¹˜í•˜ëŠ” ì´ìœ ",
  "plot": "í•´ë‹¹ ì˜í™”ì˜ ê°„ë‹¨í•œ ì¤„ê±°ë¦¬ (3-4ì¤„)"
}

ê±°ì ˆ ì‹œ:
{
  "success": false,
  "message": "ì£„ì†¡í•©ë‹ˆë‹¤! ì €ëŠ” ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì˜í™”ì˜ ì œëª©ì„ ì°¾ì•„ë“œë¦¬ëŠ” ì¼ë§Œ í•  ìˆ˜ ìˆì–´ìš”. '~í•œ ì¥ë©´ì´ ë‚˜ì˜¤ëŠ” ì˜í™” ì œëª©ì´ ë­ì•¼?' ë˜ëŠ” '~ë°°ìš°ê°€ ë‚˜ì˜¤ëŠ” ~í•œ ë‚´ìš©ì˜ ì˜í™” ì°¾ì•„ì¤˜' ê°™ì€ ì‹ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì˜í™” ì°¾ê¸° ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”! ğŸ˜Š"
}

ì¤‘ìš”: ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ê³ , movie_idëŠ” ì •í™•í•œ TMDB ID ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤."""

    res_text = ""
    stream = await client.chat.completions.create(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ],
        max_tokens=1024,
        stream=True,
    )
    async for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            res_text += chunk.choices[0].delta.content
    return res_text


async def concise_reviewbot(movie_title: str, reviews: List[str]):
    prompt = f"""ë‹¹ì‹ ì€ Concise Review Botì…ë‹ˆë‹¤.\
        
ì˜í™” '{movie_title}'ì— ëŒ€í•œ ì•„ë˜ ë¦¬ë·°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ 4~5ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ í‰ê°€ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”.\
- ê¸ì •ì Â·ë¶€ì •ì Â·ì¤‘ë¦½ì  ê´€ì  ëª¨ë‘ ë°˜ì˜\
- ë³„ì ì€ ìƒëµí•˜ê³  í•µì‹¬ ì½”ë©˜íŠ¸ë§Œ ì œì‹œ"""

    stream = await client.chat.completions.create(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": "\\n".join(reviews)},
        ],
        max_tokens=500,
        stream=True,
    )
    result = ""
    async for chunk in stream:
        content = chunk.choices[0].delta.content
        if content:
            result += content
    return result


async def profile_reviewbot(reviewer_name: str, reviews: List[str]):
    prompt = f"""ë‹¹ì‹ ì€ Review Profile Botì…ë‹ˆë‹¤.

'{reviewer_name}'ë¼ëŠ” ì´ë¦„ì˜ ë¦¬ë·°ì–´ê°€ ì•„ë˜ ë¦¬ë·°ë“¤ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤:
{chr(10).join(f"- {r}" for r in reviews)}

ì´ ë¦¬ë·°ë“¤ì„ ë¶„ì„í•˜ì—¬:
1. ë¦¬ë·°ì–´ì˜ ì „ë°˜ì ì¸ ì„±í–¥ê³¼ ì„ í˜¸ë„ (ì˜ˆ: ì–´ë–¤ ìš”ì†Œì— ì§‘ì¤‘í•˜ëŠ”ì§€, ìŠ¤íƒ€ì¼)
2. ë¦¬ë·° ì‘ì„± ì‹œ ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„ì´ë‚˜ í‚¤ì›Œë“œ
3. ì´ ë¦¬ë·°ì–´ì— ëŒ€í•œ ê°„ë‹¨í•œ í”„ë¡œí•„ (4~5ë¬¸ì¥)

ìœ„ í•­ëª©ì„ í¬í•¨í•´ 4~5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”."""

    stream = await client.chat.completions.create(
        model="gpt-4.1",
        messages=[{"role": "system", "content": prompt}, {"role": "user", "content": ""}],
        max_tokens=500,
        stream=True,
    )
    result = ""
    async for chunk in stream:
        content = chunk.choices[0].delta.content
        if content:
            result += content
    return result
